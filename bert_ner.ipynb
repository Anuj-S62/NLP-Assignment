{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujsolanki/nlp-assignment/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n",
      "Label dictionary: {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "print(dataset)\n",
    "\n",
    "# Define label mapping (ensure it matches the dataset)\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_dict = {label: i for i, label in enumerate(label_list)}\n",
    "label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "print(\"Label dictionary:\", label_dict)\n",
    "\n",
    "def preprocess_data(data, max_samples=None):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    \n",
    "    # Use all data if max_samples is None\n",
    "    sample_count = len(data) if max_samples is None else min(max_samples, len(data))\n",
    "    \n",
    "    for i in range(sample_count):\n",
    "        item = data[i]\n",
    "        token = item[\"tokens\"]\n",
    "        # converting token to lowercase\n",
    "        token = [t.lower() for t in\n",
    "                 token]\n",
    "        \n",
    "        # We shouldn't remove the stopwords as they are important for NER and change the order of the words and ner_tags\n",
    "        ner_tags = item[\"ner_tags\"]\n",
    "        tokens.append(token)\n",
    "        labels.append(ner_tags)\n",
    "    \n",
    "    return tokens, labels\n",
    "\n",
    "train_tokens, train_labels = preprocess_data(dataset[\"train\"], max_samples=2500)\n",
    "test_tokens, test_labels = preprocess_data(dataset[\"test\"], max_samples=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2500 training examples\n",
      "Processed 500 test examples\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "num_labels = len(label_dict)\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Hyperparameters\n",
    "max_length = 150\n",
    "batch_size = 16\n",
    "learning_rate = 5e-5  # Lower learning rate (standard for BERT fine-tuning)\n",
    "num_epochs = 20\n",
    "\n",
    "def encode_tokens_and_labels(tokenizer, tokens_list, labels_list, max_length):\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list_encoded = []\n",
    "    \n",
    "    for tokens, labels in zip(tokens_list, labels_list):\n",
    "        # Skip examples that are too long\n",
    "        if len(tokens) > max_length - 2:  # Account for [CLS] and [SEP]\n",
    "            continue\n",
    "            \n",
    "        # BERT tokenization may split words into subwords\n",
    "        # We need to track this to align labels correctly\n",
    "        word_ids = []\n",
    "        subwords = []\n",
    "        subword_ids = []\n",
    "        subword_attention_mask = []\n",
    "        \n",
    "        # Add [CLS] token\n",
    "        subwords.append(\"[CLS]\")\n",
    "        subword_ids.append(tokenizer.convert_tokens_to_ids(\"[CLS]\"))\n",
    "        subword_attention_mask.append(1)\n",
    "        word_ids.append(None)\n",
    "        \n",
    "        # Process each word\n",
    "        for word_idx, (word, label) in enumerate(zip(tokens, labels)):\n",
    "            # Tokenize word into subwords\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:  # Handle empty tokenization edge case\n",
    "                word_tokens = [tokenizer.unk_token]\n",
    "                \n",
    "            # Add subwords to lists\n",
    "            for i, subword in enumerate(word_tokens):\n",
    "                subwords.append(subword)\n",
    "                subword_ids.append(tokenizer.convert_tokens_to_ids(subword))\n",
    "                subword_attention_mask.append(1)\n",
    "                word_ids.append(word_idx)\n",
    "        \n",
    "        # Add [SEP] token\n",
    "        subwords.append(\"[SEP]\")\n",
    "        subword_ids.append(tokenizer.convert_tokens_to_ids(\"[SEP]\"))\n",
    "        subword_attention_mask.append(1)\n",
    "        word_ids.append(None)\n",
    "        \n",
    "        # Pad sequences\n",
    "        padding_length = max_length - len(subword_ids)\n",
    "        subword_ids.extend([0] * padding_length)\n",
    "        subword_attention_mask.extend([0] * padding_length)\n",
    "        word_ids.extend([None] * padding_length)\n",
    "        \n",
    "        # Align labels with subwords\n",
    "        label_ids = []\n",
    "        prev_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens get -100 label (ignored in loss calculation)\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != prev_word_idx:\n",
    "                # First subword of a word gets the word's label\n",
    "                label_ids.append(labels[word_idx])\n",
    "                prev_word_idx = word_idx\n",
    "            else:\n",
    "                # Subsequent subwords of the same word get -100 (ignored)\n",
    "                label_ids.append(-100)\n",
    "        \n",
    "        input_ids_list.append(subword_ids)\n",
    "        attention_mask_list.append(subword_attention_mask)\n",
    "        labels_list_encoded.append(label_ids)\n",
    "    \n",
    "    return input_ids_list, attention_mask_list, labels_list_encoded\n",
    "\n",
    "# Encode the data properly\n",
    "train_inputs, train_masks, train_labels_encoded = encode_tokens_and_labels(\n",
    "    tokenizer, train_tokens, train_labels, max_length\n",
    ")\n",
    "test_inputs, test_masks, test_labels_encoded = encode_tokens_and_labels(\n",
    "    tokenizer, test_tokens, test_labels, max_length\n",
    ")\n",
    "\n",
    "print(f\"Processed {len(train_inputs)} training examples\")\n",
    "print(f\"Processed {len(test_inputs)} test examples\")\n",
    "\n",
    "# Create tensors and dataloaders\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "train_labels = torch.tensor(train_labels_encoded)\n",
    "\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "test_labels = torch.tensor(test_labels_encoded)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujsolanki/nlp-assignment/.venv/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 157/157 [12:39<00:00,  4.84s/it, loss=0.0498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete. Average training loss: 0.3050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 157/157 [11:11<00:00,  4.28s/it, loss=0.0030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete. Average training loss: 0.0515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 157/157 [10:08<00:00,  3.87s/it, loss=0.0739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete. Average training loss: 0.0222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 157/157 [10:16<00:00,  3.93s/it, loss=0.0011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 complete. Average training loss: 0.0201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 157/157 [10:31<00:00,  4.02s/it, loss=0.0021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 complete. Average training loss: 0.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 157/157 [10:49<00:00,  4.14s/it, loss=0.0010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 complete. Average training loss: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 157/157 [10:52<00:00,  4.16s/it, loss=0.0009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 complete. Average training loss: 0.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 157/157 [12:26<00:00,  4.76s/it, loss=0.0040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 complete. Average training loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 157/157 [12:46<00:00,  4.88s/it, loss=0.0008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 complete. Average training loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 157/157 [12:28<00:00,  4.77s/it, loss=0.0008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 complete. Average training loss: 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 157/157 [12:51<00:00,  4.92s/it, loss=0.0007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 complete. Average training loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 157/157 [11:12<00:00,  4.29s/it, loss=0.0006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 complete. Average training loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 157/157 [13:07<00:00,  5.02s/it, loss=0.0005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 complete. Average training loss: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 157/157 [13:18<00:00,  5.09s/it, loss=0.0026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 complete. Average training loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 157/157 [14:32<00:00,  5.56s/it, loss=0.0006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 complete. Average training loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 157/157 [14:57<00:00,  5.72s/it, loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 complete. Average training loss: 0.0057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 157/157 [15:59<00:00,  6.11s/it, loss=0.0743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 complete. Average training loss: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 157/157 [16:03<00:00,  6.14s/it, loss=0.0005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 complete. Average training loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 157/157 [16:28<00:00,  6.29s/it, loss=0.2505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 complete. Average training loss: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 157/157 [16:30<00:00,  6.31s/it, loss=0.0013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 complete. Average training loss: 0.0109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Use proper optimization settings for BERT\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids, attention_mask, labels = [t.to(device) for t in batch]\n",
    "        \n",
    "        # Clear gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass - use BERT's built-in loss calculation\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} complete. Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('fine_tuned_ner_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:47<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2897\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.91      0.93      0.92       288\n",
      "        MISC       0.53      0.80      0.63        65\n",
      "         ORG       0.89      0.74      0.81       188\n",
      "         PER       0.96      0.98      0.97       442\n",
      "\n",
      "   micro avg       0.89      0.91      0.90       983\n",
      "   macro avg       0.82      0.86      0.83       983\n",
      "weighted avg       0.90      0.91      0.90       983\n",
      "\n",
      "\n",
      "Sample Predictions:\n",
      "Example 1:\n",
      "True: ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O']\n",
      "Pred: ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Example 2:\n",
      "True: ['B-PER', 'I-PER']\n",
      "Pred: ['B-PER', 'I-PER']\n",
      "\n",
      "Example 3:\n",
      "True: ['B-LOC', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O']\n",
      "Pred: ['B-LOC', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O']\n",
      "\n",
      "Example 4:\n",
      "True: ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Pred: ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Example 5:\n",
      "True: ['O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']\n",
      "Pred: ['O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids, attention_mask, labels = [t.to(device) for t in batch]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "    \n",
    "    test_loss += outputs.loss.item()\n",
    "    \n",
    "    # Get predictions\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    \n",
    "    # Move predictions and labels to CPU\n",
    "    predictions = predictions.detach().cpu().numpy()\n",
    "    labels = labels.to('cpu').numpy()\n",
    "    \n",
    "    # Store predictions and true labels\n",
    "    for i in range(predictions.shape[0]):\n",
    "        pred_ids = predictions[i]\n",
    "        label_ids = labels[i]\n",
    "        \n",
    "        # Filter out ignored indices (-100)\n",
    "        mask = label_ids != -100\n",
    "        true_label_ids = label_ids[mask]\n",
    "        pred_label_ids = pred_ids[mask]\n",
    "        \n",
    "        all_predictions.append([label_dict_inverse[id] for id in pred_label_ids])\n",
    "        all_true_labels.append([label_dict_inverse[id] for id in true_label_ids])\n",
    "\n",
    "# Calculate average test loss\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "print(f\"Test loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_true_labels, all_predictions))\n",
    "\n",
    "# Print some example predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(min(5, len(all_predictions))):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"True: {all_true_labels[i]}\")\n",
    "    print(f\"Pred: {all_predictions[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an (B-PER) ##uj (B-PER) is very great developer at the linux (B-ORG) foundation (I-ORG) from india (B-LOC) \n"
     ]
    }
   ],
   "source": [
    "# load the saved model\n",
    "model = BertForTokenClassification.from_pretrained('fine_tuned_ner_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess(text):\n",
    "    if(isinstance(text, list)):\n",
    "        text = ' '.join(text)\n",
    "    tokens = []\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "def predict(text):\n",
    "    tokens = preprocess(text)\n",
    "    input_ids = tokenizer.encode(tokens, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    predictions = predictions[:, 1:]\n",
    "    predicted_label_ids = predictions[0].numpy()\n",
    "    predicted_labels = [label_dict_inverse[id] for id in predicted_label_ids]\n",
    "    return list(zip(tokens, predicted_labels))\n",
    "\n",
    "def display_predictions(predictions):\n",
    "    # annotate the text with the predicted labels\n",
    "    annotated_text = \"\"\n",
    "    for token, label in predictions:\n",
    "        if(label=='O'):\n",
    "            annotated_text += f\"{token} \"\n",
    "        else:\n",
    "            annotated_text += f\"{token} ({label}) \"\n",
    "    print(annotated_text)\n",
    "\n",
    "# Test the model\n",
    "text = \"Anuj is very great developer at the Arist Networks from India\"\n",
    "# text = dataset[\"train\"][0][\"tokens\"]\n",
    "predictions = predict(text)\n",
    "# print(predictions)\n",
    "display_predictions(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
