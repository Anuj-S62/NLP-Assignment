# Named Entity Recognition using BERT and spaCy app

### üåê Test the App Yourself  
You can try out the NER application here: [https://anuj-ner-app.hdebian.online/](https://anuj-ner-app.hdebian.online/)  

## Run the app (locally):
Clone the repository <br>
```
git clone https://github.com/Anuj-S62/NLP-Assignment.git
```
Size of BERT model is 467 MB which is larger the allowed file size on github <br>
Download fined-tuned BERT model [G-Drive](https://drive.google.com/drive/folders/1A8--OsHyaOeUmSWR1NxG3mx-ByMfjCt5?usp=drive_link) <br>
Copy the BERT Model to project root directory <br>

```
cd <project-root-directory>
```
Set the secure password in both .env files in ./frontend and in ./ <br>

### Run app using Docker:<br>

Install docker <br>

Run the following docker command to build and run the app locally

```bash
docker compose up -d
```
To stop the service:
```bash
docker compose down
```

### Run app locally without docker
Create and activate virtual environment<br>
```
python3 -m venv .venv
source .venv/bin/activate
```
Install dependencies
```
pip install -r requirements.txt

```
Start backend
```
uvicorn app:app --host 0.0.0.0 --port 8000
```
Run Frontend (Streamlit)
```
cd ./frontend
streamlit run frontend.py --server.port=8501
```

## Data Preprocessing and Feature Engineering

### Overview
For this project, we developed and trained Named Entity Recognition (NER) models using **BERT** and **spaCy**. The preprocessing steps ensured that the text data was cleaned, tokenized, and converted into a structured format suitable for model training.

---

### Steps Taken for Data Preprocessing

#### 1. Loading the Dataset
- The dataset was loaded and analyzed using **Exploratory Data Analysis (EDA)**.
- Checked for missing values and distribution of entity labels.

#### 2. Preprocessing for BERT-based NER Model
- **Tokenization**: Used the pre-trained tokenizer from BERT to split text into subwords.
- **Lowercasing**: Converted all tokens to lowercase for consistency.
- **Stopword Handling**: Unlike traditional NLP tasks, stopwords were **not removed**, as they are crucial for NER.
- **Entity Label Alignment**: Adjusted entity labels to align with subwords generated by the BERT tokenizer.
- **Padding and Truncation**: Sequences were padded to a fixed `max_length` to ensure uniform input sizes.

#### 3. Preprocessing for spaCy-based NER Model
- **Custom Tokenization**: Used spaCy‚Äôs tokenizer to segment words.
- **Entity Labeling**: Assigned correct entity labels from the dataset.
- **Training Data Formatting**: Converted the dataset into spaCy‚Äôs required format of `(text, {"entities": [...]})`.

---

### Feature Engineering

#### 1. Feature Engineering for BERT Model
- **Input Features**:
  - `input_ids`: Tokenized and indexed representation of words.
  - `attention_mask`: Specifies which tokens are actual words (1) and which are padding (0).
  - `labels`: Entity tags aligned with tokenized words.
- **Subword Handling**:
  - Some words are split into multiple subwords by BERT‚Äôs tokenizer.
  - Labels were carefully aligned to handle subwords correctly.
- **Model Optimization**:
  - Used **AdamW optimizer** for better weight updates.
  - Fine-tuned BERT for multiple epochs with batch training.

#### 2. Feature Engineering for spaCy Model
- **Custom Named Entity Recognition (NER) Pipeline**:
  - Added a custom `ner` component to the spaCy pipeline.
  - Registered **unique entity labels** from the dataset.
- **Mini-batching and Shuffling**:
  - Used minibatch training to improve model generalization.
  - Shuffled training data between epochs to prevent overfitting.
- **Dropout Regularization**:
  - Applied **dropout (0.2)** to prevent overfitting.

---

### 4. Entity Labels  

- **O (Outside)**: The token does not belong to any named entity.  
- **B-PER (Beginning of Person Name)**: Marks the beginning of a person‚Äôs name.  
- **I-PER (Inside Person Name)**: Indicates a continuation of a person‚Äôs name.  
- **B-ORG (Beginning of Organization Name)**: Marks the beginning of an organization‚Äôs name.  
- **I-ORG (Inside Organization Name)**: Indicates a continuation of an organization‚Äôs name.  
- **B-LOC (Beginning of Location Name)**: Marks the beginning of a location‚Äôs name (e.g., city, country).  
- **I-LOC (Inside Location Name)**: Indicates a continuation of a location‚Äôs name.  
- **B-MISC (Beginning of Miscellaneous Entity)**: Marks the beginning of a miscellaneous entity (e.g., nationalities, events).  
- **I-MISC (Inside Miscellaneous Entity)**: Indicates a continuation of a miscellaneous entity.  

---

### Summary
- Both **BERT-based** and **spaCy-based** models were trained with optimized preprocessing.
- Entity labels were carefully **aligned with tokenized words**, ensuring high accuracy.
- **Feature engineering** was done by adjusting subwords, padding, and training parameters.

This preprocessing pipeline ensures that the dataset is structured, optimized, and ready for effective Named Entity Recognition.


## Bonus Tasks Completed  

- Implemented **Named Entity Recognition (NER)** using both **BERT** and **SpaCy** models, enabling flexible entity extraction.  
- Developed a **Streamlit-based frontend** for interactive testing and visualization of NER results.  
- Containerized the application using **Docker**, including both the FastAPI backend and Streamlit frontend.  
- Successfully **deployed the application** on my **self-hosted computer** for local and remote access using **Cloudflare Tunnel**, ensuring secure and seamless access without exposing ports.

- # Screenshots
- Using BERT
<img width="1434" alt="Screenshot 2025-03-13 at 12 44 10‚ÄØAM" src="https://github.com/user-attachments/assets/5461e5e0-2c62-44d8-8c1d-9e97887527e6" />

<br>
- Using spaCy
<img width="1434" alt="Screenshot 2025-03-13 at 12 44 33‚ÄØAM" src="https://github.com/user-attachments/assets/c5fb4f1f-62a8-4e68-8863-ce8d8fc6aea3" />


