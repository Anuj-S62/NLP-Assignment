{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n",
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "print(\"Dataset structure:\", dataset)\n",
    "\n",
    "# Define NER label mapping (from numeric to string format that spaCy uses)\n",
    "# Get label names directly from the dataset\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(\"Labels:\", label_list)\n",
    "\n",
    "# Function to convert tokens and tags to spaCy examples\n",
    "def create_spacy_examples(nlp, dataset_split, max_samples=None):\n",
    "    examples = []\n",
    "    \n",
    "    sample_count = len(dataset_split) if max_samples is None else min(max_samples, len(dataset_split))\n",
    "    \n",
    "    for i in tqdm(range(sample_count), desc=f\"Processing {sample_count} examples\"):\n",
    "        item = dataset_split[i]\n",
    "        tokens = item[\"tokens\"]\n",
    "        ner_tags = item[\"ner_tags\"]\n",
    "        \n",
    "        # Create a spaCy Doc with the tokens\n",
    "        # We need to set proper spaces to ensure correct character offsets\n",
    "        spaces = [True] * len(tokens)\n",
    "        if spaces:  # Make sure the last token doesn't have a trailing space\n",
    "            spaces[-1] = False\n",
    "        \n",
    "        doc = Doc(nlp.vocab, words=tokens, spaces=spaces)\n",
    "        \n",
    "        # Extract entities based on BIO tags\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        \n",
    "        for token_idx, (token, tag_id) in enumerate(zip(doc, ner_tags)):\n",
    "            tag = label_list[tag_id]\n",
    "            \n",
    "            # Starting a new entity\n",
    "            if tag.startswith(\"B-\"):\n",
    "                if current_entity is not None:\n",
    "                    # Add the previous entity\n",
    "                    entities.append(current_entity)\n",
    "                # Start a new entity\n",
    "                current_entity = {\n",
    "                    \"start\": token.idx,\n",
    "                    \"end\": token.idx + len(token.text),\n",
    "                    \"label\": tag[2:]  # Remove the \"B-\" prefix\n",
    "                }\n",
    "            # Inside an entity\n",
    "            elif tag.startswith(\"I-\") and current_entity is not None:\n",
    "                # Only extend if it's the same entity type\n",
    "                if current_entity[\"label\"] == tag[2:]:\n",
    "                    current_entity[\"end\"] = token.idx + len(token.text)\n",
    "                else:\n",
    "                    # If entity type has changed, add the previous one and start a new one\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = {\n",
    "                        \"start\": token.idx,\n",
    "                        \"end\": token.idx + len(token.text),\n",
    "                        \"label\": tag[2:]\n",
    "                    }\n",
    "            # Outside any entity\n",
    "            elif tag == \"O\":\n",
    "                if current_entity is not None:\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "        \n",
    "        # Add any entity that's still open at the end\n",
    "        if current_entity is not None:\n",
    "            entities.append(current_entity)\n",
    "        \n",
    "        # Create the entity spans dict for the Example object\n",
    "        spans = {\"entities\": [(ent[\"start\"], ent[\"end\"], ent[\"label\"]) for ent in entities]}\n",
    "        \n",
    "        # Create and add the example\n",
    "        examples.append(Example.from_dict(doc, spans))\n",
    "    \n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a custom NER model\n",
    "def train_spacy_model(train_examples, validation_examples=None):\n",
    "    # Load a blank English model\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    \n",
    "    # Create a new NER component and add it to the pipeline\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\", last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # Add entity labels from our dataset\n",
    "    for label in set([label[2:] for label in label_list if label.startswith(\"B-\")]):\n",
    "        ner.add_label(label)\n",
    "    \n",
    "    # Configure training\n",
    "    n_iter = 30\n",
    "    \n",
    "    # Initialize the optimizer\n",
    "    optimizer = nlp.initialize(lambda: train_examples)\n",
    "    \n",
    "    # Training loop\n",
    "    batch_sizes = compounding(4.0, 32.0, 1.001)  # Gradually increase batch size\n",
    "    \n",
    "    # Store metrics for each epoch\n",
    "    training_losses = []\n",
    "    validation_metrics = []\n",
    "    \n",
    "    print(f\"Training with {len(train_examples)} examples\")\n",
    "    print(f\"Validation with {len(validation_examples) if validation_examples else 0} examples\")\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Shuffle training data\n",
    "        random.shuffle(train_examples)\n",
    "        \n",
    "        # Create batches and train\n",
    "        losses = {}\n",
    "        batches = minibatch(train_examples, size=batch_sizes)\n",
    "        \n",
    "        # Training step\n",
    "        for batch in tqdm(list(batches), desc=f\"Epoch {i+1}/{n_iter}\"):\n",
    "            nlp.update(batch, drop=0.2, losses=losses)\n",
    "        \n",
    "        # Track training loss\n",
    "        epoch_loss = losses.get(\"ner\", 0)\n",
    "        training_losses.append(epoch_loss)\n",
    "        print(f\"Epoch {i+1}/{n_iter}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "    return nlp, {\"training_losses\": training_losses, \"validation_metrics\": validation_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 14041 examples: 100%|██████████| 14041/14041 [00:04<00:00, 3195.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 500 examples: 100%|██████████| 500/500 [00:00<00:00, 3028.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 500 examples: 100%|██████████| 500/500 [00:00<00:00, 3866.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model...\n",
      "Training with 14041 examples\n",
      "Validation with 500 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 1548/1548 [01:39<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 14734.8828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 567/567 [01:01<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Loss: 7467.6943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 439/439 [00:45<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Loss: 5149.2837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 439/439 [00:45<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Loss: 4257.0566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 439/439 [00:45<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Loss: 3407.7698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 439/439 [00:45<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Loss: 2972.8726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 439/439 [00:46<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Loss: 2608.0769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 439/439 [00:45<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Loss: 2452.7244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 439/439 [00:49<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Loss: 2352.5393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 439/439 [01:13<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Loss: 1928.2369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 439/439 [01:21<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Loss: 1905.8416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 439/439 [01:31<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Loss: 1701.5107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 439/439 [00:49<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Loss: 1561.4275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 439/439 [00:45<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Loss: 1575.1489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 439/439 [01:16<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Loss: 1426.1211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 439/439 [01:11<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30, Loss: 1352.1597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 439/439 [01:12<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30, Loss: 1376.2960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 439/439 [01:00<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30, Loss: 1154.5421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 439/439 [00:57<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30, Loss: 1186.5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 439/439 [01:14<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30, Loss: 1254.2452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 439/439 [01:18<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30, Loss: 1067.8771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 439/439 [01:14<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30, Loss: 1019.4557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 439/439 [01:14<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30, Loss: 970.7191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|██████████| 439/439 [01:12<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Loss: 944.7495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 439/439 [01:14<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30, Loss: 892.5607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 439/439 [01:14<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30, Loss: 978.9185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 439/439 [01:13<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30, Loss: 878.4814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|██████████| 439/439 [01:14<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30, Loss: 809.5173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|██████████| 439/439 [01:13<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30, Loss: 830.5732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|██████████| 439/439 [01:14<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Loss: 750.6155\n",
      "Model saved to ./models/conll_ner_model\n"
     ]
    }
   ],
   "source": [
    "# Create a blank spaCy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create examples from the dataset\n",
    "print(\"Creating training examples...\")\n",
    "train_examples = create_spacy_examples(nlp, dataset[\"train\"], max_samples=None)\n",
    "\n",
    "print(\"Creating validation examples...\")\n",
    "validation_examples = create_spacy_examples(nlp, dataset[\"validation\"], max_samples=500)\n",
    "\n",
    "print(\"Creating test examples...\")\n",
    "test_examples = create_spacy_examples(nlp, dataset[\"test\"], max_samples=500)\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining the model...\")\n",
    "trained_model, metrics = train_spacy_model(train_examples, validation_examples)\n",
    "\n",
    "# Save the model\n",
    "if not os.path.exists(\"./models\"):\n",
    "    os.makedirs(\"./models\")\n",
    "\n",
    "trained_model.to_disk(\"./models/conll_ner_model\")\n",
    "print(\"Model saved to ./models/conll_ner_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate model on examples\n",
    "def evaluate_examples(nlp, examples):\n",
    "    tp = fp = fn = 0\n",
    "    \n",
    "    # Entity-wise evaluation\n",
    "    entity_scores = {}\n",
    "    \n",
    "    for ex in examples:\n",
    "        # Check if 'entities' exists in spans, if not skip this example\n",
    "        if \"entities\" not in ex.reference.spans:\n",
    "            continue\n",
    "            \n",
    "        gold_entities = set([(e[0], e[1], e[2]) for e in ex.reference.spans[\"entities\"]])\n",
    "        \n",
    "        # Get predictions\n",
    "        pred_doc = nlp(ex.reference.text)\n",
    "        pred_entities = set([(e.start_char, e.end_char, e.label_) for e in pred_doc.ents])\n",
    "        \n",
    "        # Update counts\n",
    "        for entity in gold_entities:\n",
    "            if entity in pred_entities:\n",
    "                tp += 1\n",
    "                \n",
    "                # Entity-specific metrics\n",
    "                if entity[2] not in entity_scores:\n",
    "                    entity_scores[entity[2]] = {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
    "                entity_scores[entity[2]][\"tp\"] += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "                \n",
    "                # Entity-specific metrics\n",
    "                if entity[2] not in entity_scores:\n",
    "                    entity_scores[entity[2]] = {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
    "                entity_scores[entity[2]][\"fn\"] += 1\n",
    "        \n",
    "        for entity in pred_entities:\n",
    "            if entity not in gold_entities:\n",
    "                fp += 1\n",
    "                \n",
    "                # Entity-specific metrics\n",
    "                if entity[2] not in entity_scores:\n",
    "                    entity_scores[entity[2]] = {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
    "                entity_scores[entity[2]][\"fp\"] += 1\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate entity-specific metrics\n",
    "    for entity_type, counts in entity_scores.items():\n",
    "        entity_tp = counts[\"tp\"]\n",
    "        entity_fp = counts[\"fp\"]\n",
    "        entity_fn = counts[\"fn\"]\n",
    "        \n",
    "        entity_precision = entity_tp / (entity_tp + entity_fp) if (entity_tp + entity_fp) > 0 else 0\n",
    "        entity_recall = entity_tp / (entity_tp + entity_fn) if (entity_tp + entity_fn) > 0 else 0\n",
    "        entity_f1 = 2 * entity_precision * entity_recall / (entity_precision + entity_recall) if (entity_precision + entity_recall) > 0 else 0\n",
    "        \n",
    "        entity_scores[entity_type][\"precision\"] = entity_precision\n",
    "        entity_scores[entity_type][\"recall\"] = entity_recall\n",
    "        entity_scores[entity_type][\"f1\"] = entity_f1\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"entity_scores\": entity_scores\n",
    "    }\n",
    "\n",
    "# Display detailed evaluation results\n",
    "def display_evaluation_results(scores):\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Overall Precision: {scores['precision']:.4f}\")\n",
    "    print(f\"Overall Recall: {scores['recall']:.4f}\")\n",
    "    print(f\"Overall F1 Score: {scores['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nEntity-specific scores:\")\n",
    "    for entity_type, metrics in scores[\"entity_scores\"].items():\n",
    "        print(f\"\\n{entity_type}:\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1 Score: {metrics['f1']:.4f}\")\n",
    "\n",
    "# Function to display example predictions\n",
    "def show_example_predictions(nlp, examples, num_examples=5):\n",
    "    print(\"\\n--- Example Predictions ---\")\n",
    "    \n",
    "    for i, example in enumerate(examples[:num_examples]):\n",
    "        text = example.reference.text\n",
    "        # gold_entities = [(text[start:end], label) for start, end, label in example.reference.spans[\"entities\"]]\n",
    "        \n",
    "        # Get predictions\n",
    "        doc = nlp(text)\n",
    "        pred_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Text: {text}\")\n",
    "        # print(\"Gold entities:\", gold_entities)\n",
    "        print(\"Predicted entities:\", pred_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2000 examples: 100%|██████████| 2000/2000 [00:00<00:00, 3568.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 500 examples: 100%|██████████| 500/500 [00:00<00:00, 4648.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 500 examples: 100%|██████████| 500/500 [00:00<00:00, 5012.48it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a blank spaCy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create examples from the dataset\n",
    "print(\"Creating training examples...\")\n",
    "train_examples = create_spacy_examples(nlp, dataset[\"train\"], max_samples=2000)\n",
    "\n",
    "print(\"Creating validation examples...\")\n",
    "validation_examples = create_spacy_examples(nlp, dataset[\"validation\"], max_samples=500)\n",
    "\n",
    "print(\"Creating test examples...\")\n",
    "test_examples = create_spacy_examples(nlp, dataset[\"test\"], max_samples=500)\n",
    "\n",
    "# # Train the model\n",
    "# print(\"\\nTraining the model...\")\n",
    "# trained_model, metrics = train_spacy_model(train_examples, validation_examples)\n",
    "\n",
    "# # Save the model\n",
    "# if not os.path.exists(\"./models\"):\n",
    "#     os.makedirs(\"./models\")\n",
    "\n",
    "# trained_model.to_disk(\"./models/conll_ner_model\")\n",
    "# print(\"Model saved to ./models/conll_ner_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test data...\n",
      "\n",
      "--- Evaluation Results ---\n",
      "Overall Precision: 0.0000\n",
      "Overall Recall: 0.0000\n",
      "Overall F1 Score: 0.0000\n",
      "\n",
      "Entity-specific scores:\n",
      "\n",
      "--- Example Predictions ---\n",
      "\n",
      "Example 1:\n",
      "Text: SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .\n",
      "Predicted entities: [('JAPAN', 'PER'), ('LUCKY', 'PER'), ('CHINA', 'ORG')]\n",
      "\n",
      "Example 2:\n",
      "Text: Nadim Ladki\n",
      "Predicted entities: [('Nadim Ladki', 'PER')]\n",
      "\n",
      "Example 3:\n",
      "Text: AL-AIN , United Arab Emirates 1996-12-06\n",
      "Predicted entities: [('AL', 'LOC'), ('AIN', 'ORG'), ('United Arab Emirates', 'PER')]\n",
      "\n",
      "Example 4:\n",
      "Text: Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday .\n",
      "Predicted entities: [('Japan', 'LOC'), ('Asian Cup', 'MISC'), ('Syria', 'LOC')]\n",
      "\n",
      "Example 5:\n",
      "Text: But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan .\n",
      "Predicted entities: [('China', 'LOC'), ('Uzbekistan', 'LOC')]\n",
      "\n",
      "--- Custom Text Prediction ---\n",
      "Text: John Smith from Google visited New York last week for a conference about AI technology.\n",
      "Predicted entities:\n",
      "  John Smith (PER)\n",
      "  Google (LOC)\n",
      "  New York (LOC)\n",
      "  AI (MISC)\n"
     ]
    }
   ],
   "source": [
    "# load saved model\n",
    "trained_model = spacy.load(\"./models/conll_ner_model\")\n",
    "# Evaluate on test data\n",
    "print(\"\\nEvaluating on test data...\")\n",
    "test_scores = evaluate_examples(trained_model, test_examples)\n",
    "display_evaluation_results(test_scores)\n",
    "\n",
    "# Show some example predictions\n",
    "show_example_predictions(trained_model, test_examples)\n",
    "\n",
    "# Function to test on custom text\n",
    "def test_ner(text):\n",
    "    doc = trained_model(text)\n",
    "    print(\"\\n--- Custom Text Prediction ---\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Predicted entities:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"  {ent.text} ({ent.label_})\")\n",
    "\n",
    "# Test on a custom text\n",
    "test_ner(\"John Smith from Google visited New York last week for a conference about AI technology.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Custom Text Prediction ---\n",
      "Text: Apple Inc. announced the launch of its latest iPhone 15 in California on September 12, 2023. CEO Tim Cook highlighted new features, including an advanced A17 Bionic chip and improved camera technology. Analysts predict that the device will boost Apple's market share, especially in the United States and Europe.\n",
      "Predicted entities:\n",
      "  Apple Inc. (ORG)\n",
      "  California (LOC)\n",
      "  Tim Cook (PER)\n",
      "  A17 Bionic (PER)\n",
      "  Apple (ORG)\n",
      "  United States (LOC)\n",
      "  Europe (LOC)\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi, Apple Inc. is a company in California.\"\n",
    "text = \"Apple Inc. announced the launch of its latest iPhone 15 in California on September 12, 2023. CEO Tim Cook highlighted new features, including an advanced A17 Bionic chip and improved camera technology. Analysts predict that the device will boost Apple's market share, especially in the United States and Europe.\"\n",
    "doc = trained_model(text)\n",
    "print(\"\\n--- Custom Text Prediction ---\")\n",
    "print(f\"Text: {text}\")\n",
    "print(\"Predicted entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"  {ent.text} ({ent.label_})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
