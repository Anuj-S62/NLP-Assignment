{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujsolanki/nlp-assignment/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n",
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from spacy.util import minibatch\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "print(\"Dataset structure:\", dataset)\n",
    "\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(\"Labels:\", label_list)\n",
    "\n",
    "epochs = 35\n",
    "max_samples = None\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Create a spaCy Doc with the tokens\n",
    "def create_training_examples(nlp, dataset_split, max_samples=None):\n",
    "    examples = []\n",
    "    \n",
    "    sample_count = len(dataset_split) if max_samples is None else min(max_samples, len(dataset_split))\n",
    "    \n",
    "    for i in tqdm(range(sample_count), desc=f\"Processing {sample_count} examples\"):\n",
    "        item = dataset_split[i]\n",
    "        tokens = item[\"tokens\"]\n",
    "        ner_tags = item[\"ner_tags\"]\n",
    "        \n",
    "        spaces = [True] * len(tokens)\n",
    "        if spaces:  \n",
    "            spaces[-1] = False\n",
    "        \n",
    "        doc = Doc(nlp.vocab, words=tokens, spaces=spaces)\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        \n",
    "        for token_idx, (token, tag_id) in enumerate(zip(doc, ner_tags)):\n",
    "            tag = label_list[tag_id]\n",
    "            \n",
    "            if tag.startswith(\"B-\") or tag.startswith(\"I-\") or tag == \"O\":\n",
    "                if current_entity is not None:\n",
    "                    # Add the previous entity\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "                # Start a new entity\n",
    "                current_entity = {\n",
    "                    \"start\": token.idx,\n",
    "                    \"end\": token.idx + len(token.text),\n",
    "                    \"label\": tag \n",
    "                }\n",
    "        \n",
    "        if current_entity is not None:\n",
    "            entities.append(current_entity)\n",
    "        \n",
    "        reference = doc.copy()\n",
    "        for ent in entities:\n",
    "            span = reference.char_span(ent[\"start\"], ent[\"end\"], label=ent[\"label\"])\n",
    "            if span is not None:\n",
    "                reference.ents = list(reference.ents) + [span]\n",
    "        \n",
    "        # Create example\n",
    "        example = Example(doc, reference)\n",
    "        examples.append(example)\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_spacy_model(train_examples):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    \n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\", last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    unique_labels = set()\n",
    "    for label in label_list:\n",
    "        unique_labels.add(label)\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        print(f\"Adding label: {label}\")\n",
    "        ner.add_label(label)\n",
    "    \n",
    "    n_iter = epochs\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    \n",
    "    training_losses = []\n",
    "    \n",
    "    print(f\"Training with {len(train_examples)} examples\")\n",
    "    \n",
    "    # Train the model\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            random.shuffle(train_examples)\n",
    "            \n",
    "            losses = {}\n",
    "            batches = minibatch(train_examples, size=batch_size)\n",
    "            \n",
    "            for batch in tqdm(list(batches), desc=f\"Epoch {i+1}/{n_iter}\"):\n",
    "                nlp.update(batch, drop=0.2, losses=losses)\n",
    "            \n",
    "            epoch_loss = losses.get(\"ner\", 0)\n",
    "            training_losses.append(epoch_loss)\n",
    "            print(f\"Epoch {i+1}/{n_iter}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "    \n",
    "    return nlp, {\"training_losses\": training_losses}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 14041 examples: 100%|██████████| 14041/14041 [00:05<00:00, 2763.68it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create training and validation data\n",
    "print(\"Preparing training data...\")\n",
    "train_examples = create_training_examples(nlp, dataset[\"train\"], max_samples=max_samples) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Adding label: B-ORG\n",
      "Adding label: I-ORG\n",
      "Adding label: B-MISC\n",
      "Adding label: O\n",
      "Adding label: B-LOC\n",
      "Adding label: I-PER\n",
      "Adding label: I-MISC\n",
      "Adding label: I-LOC\n",
      "Adding label: B-PER\n",
      "Training with 14041 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/35: 100%|██████████| 439/439 [00:41<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35, Loss: 24167.9258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/35: 100%|██████████| 439/439 [00:42<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/35, Loss: 9244.8447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/35: 100%|██████████| 439/439 [00:44<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/35, Loss: 6270.2588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/35: 100%|██████████| 439/439 [00:45<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/35, Loss: 4702.3926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/35: 100%|██████████| 439/439 [00:43<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/35, Loss: 3676.5750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/35: 100%|██████████| 439/439 [00:44<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/35, Loss: 3160.5959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/35: 100%|██████████| 439/439 [00:48<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/35, Loss: 2689.3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/35: 100%|██████████| 439/439 [00:46<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/35, Loss: 2391.2646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/35: 100%|██████████| 439/439 [00:48<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/35, Loss: 2204.1631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/35: 100%|██████████| 439/439 [00:46<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/35, Loss: 1926.3594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/35: 100%|██████████| 439/439 [00:44<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/35, Loss: 1740.3558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/35: 100%|██████████| 439/439 [00:43<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/35, Loss: 1672.2102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/35: 100%|██████████| 439/439 [00:46<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/35, Loss: 1452.6094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/35: 100%|██████████| 439/439 [00:44<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/35, Loss: 1459.4565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/35: 100%|██████████| 439/439 [00:44<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/35, Loss: 1337.9694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/35: 100%|██████████| 439/439 [00:42<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/35, Loss: 1266.2065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/35: 100%|██████████| 439/439 [00:42<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/35, Loss: 1185.5085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/35: 100%|██████████| 439/439 [00:41<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/35, Loss: 1103.0460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/35: 100%|██████████| 439/439 [00:43<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/35, Loss: 1201.1912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/35: 100%|██████████| 439/439 [00:42<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/35, Loss: 1037.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/35: 100%|██████████| 439/439 [00:40<00:00, 10.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/35, Loss: 982.6530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/35: 100%|██████████| 439/439 [00:41<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/35, Loss: 913.4857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/35: 100%|██████████| 439/439 [00:41<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/35, Loss: 960.3520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/35: 100%|██████████| 439/439 [00:41<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/35, Loss: 952.8929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/35: 100%|██████████| 439/439 [00:42<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/35, Loss: 906.9959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/35: 100%|██████████| 439/439 [00:43<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/35, Loss: 879.9451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/35: 100%|██████████| 439/439 [00:43<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/35, Loss: 845.0806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/35: 100%|██████████| 439/439 [00:43<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/35, Loss: 854.9001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/35: 100%|██████████| 439/439 [00:41<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/35, Loss: 816.9431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/35: 100%|██████████| 439/439 [00:41<00:00, 10.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/35, Loss: 729.6969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/35: 100%|██████████| 439/439 [00:40<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/35, Loss: 692.9923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/35: 100%|██████████| 439/439 [00:41<00:00, 10.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/35, Loss: 738.2781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/35: 100%|██████████| 439/439 [00:40<00:00, 10.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/35, Loss: 727.5371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/35: 100%|██████████| 439/439 [00:40<00:00, 10.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/35, Loss: 664.5427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/35: 100%|██████████| 439/439 [00:41<00:00, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/35, Loss: 708.0412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "print(\"Training model...\")\n",
    "trained_model, metrics = train_spacy_model(train_examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to pickle\n",
    "import pickle\n",
    "with open(\"spacy_ner_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trained_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'Inc', 'is', 'an', 'American', 'multinational', 'technology', 'company', 'headquartered', 'in', 'Cupertino', ',', 'California', '.']\n",
      "['B-ORG', 'I-ORG', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "def predict(model, text):\n",
    "    doc = model(text)\n",
    "    token_entities = []\n",
    "    token = []\n",
    "    for ent in doc.ents:\n",
    "        token.append(ent.text)\n",
    "        token_entities.append(ent.label_)\n",
    "    return token,token_entities\n",
    "\n",
    "text = \"Apple Inc is an American multinational technology company headquartered in Cupertino, California.\"\n",
    "tokens,entities = predict(trained_model, text)\n",
    "print(tokens)\n",
    "print(entities)\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "test_examples = dataset[\"test\"][:500]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
      "NER Metrics:\n",
      "Label: O\n",
      "Precision: 0.8614\n",
      "Recall: 0.8657\n",
      "F1-score: 0.8635\n",
      "\n",
      "Label: B-PER\n",
      "Precision: 0.1667\n",
      "Recall: 0.1667\n",
      "F1-score: 0.1667\n",
      "\n",
      "Label: I-PER\n",
      "Precision: 0.1667\n",
      "Recall: 0.1667\n",
      "F1-score: 0.1667\n",
      "\n",
      "Label: B-ORG\n",
      "Precision: 0.1250\n",
      "Recall: 0.1429\n",
      "F1-score: 0.1333\n",
      "\n",
      "Label: I-ORG\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Label: B-LOC\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Label: I-LOC\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Label: B-MISC\n",
      "Precision: 0.3333\n",
      "Recall: 0.2857\n",
      "F1-score: 0.3077\n",
      "\n",
      "Label: I-MISC\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Label: Overall\n",
      "Precision: 0.7553\n",
      "Recall: 0.7553\n",
      "F1-score: 0.7553\n",
      "Accuracy: 0.7553\n",
      "\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def calculate_ner_metrics(true_entities, pred_entities, labels):\n",
    "    assert len(true_entities) == len(pred_entities), \"Mismatch in number of sentences\"\n",
    "\n",
    "    # Flatten lists\n",
    "    y_true = [label for seq in true_entities for label in seq]\n",
    "    y_pred = [label for seq in pred_entities for label in seq]\n",
    "\n",
    "    # Calculate TP, FP, FN for each label\n",
    "    label_counts = {label: Counter() for label in labels}\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == pred:\n",
    "            label_counts[true]['TP'] += 1\n",
    "        else:\n",
    "            label_counts[true]['FN'] += 1\n",
    "            label_counts[pred]['FP'] += 1\n",
    "\n",
    "    # Compute metrics for each label\n",
    "    metrics = {}\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "\n",
    "    for label, counts in label_counts.items():\n",
    "        tp, fp, fn = counts['TP'], counts['FP'], counts['FN']\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "        metrics[label] = {\"Precision\": precision, \"Recall\": recall, \"F1-score\": f1}\n",
    "\n",
    "    # Micro-averaged scores\n",
    "    overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = sum(1 for true, pred in zip(y_true, y_pred) if true == pred) / len(y_true)\n",
    "\n",
    "    metrics[\"Overall\"] = {\n",
    "        \"Precision\": overall_precision,\n",
    "        \"Recall\": overall_recall,\n",
    "        \"F1-score\": overall_f1,\n",
    "        \"Accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate(model, test_dataset, label_list):\n",
    "\n",
    "    \n",
    "    true_entities = []\n",
    "    pred_entities = []\n",
    "    tokens = test_dataset[\"tokens\"]\n",
    "    ner_tags = test_dataset[\"ner_tags\"]\n",
    "    x = 5\n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "        tag = ner_tags[i]\n",
    "        tag_with_label = [label_list[tag_id] for tag_id in tag]\n",
    "        text = \" \".join(token)\n",
    "        _, predicted_entities = predict(model, text)\n",
    "        true_entities.append(tag_with_label)\n",
    "        pred_entities.append(predicted_entities)\n",
    "    \n",
    "    metrics = calculate_ner_metrics(true_entities, pred_entities, label_list)\n",
    "    print(\"NER Metrics:\")\n",
    "    for label, scores in metrics.items():\n",
    "        print(f\"Label: {label}\")\n",
    "        for metric, score in scores.items():\n",
    "            print(f\"{metric}: {score:.4f}\")\n",
    "        print()\n",
    "    print(len(metrics))\n",
    "    return metrics\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open(\"spacy_ner_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "test_dataset = test_examples\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(\"Labels:\", label_list)\n",
    "metric = evaluate(model, test_dataset,label_list=label_list)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
